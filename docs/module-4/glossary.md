---
sidebar_position: 5
---

# Glossary: Vision-Language-Action (VLA) Concepts

## A

## C
### Cognitive Planning
The process of translating high-level natural language commands into specific, executable robot actions using large language models (LLMs).

## L
### LLM (Large Language Model)
A type of artificial intelligence model trained on vast amounts of text data, capable of understanding and generating human-like text, often used for natural language processing tasks.

## R
### ROS 2 (Robot Operating System 2)
A flexible framework for writing robot software that provides hardware abstraction, device drivers, libraries, visualizers, message-passing, package management, and more.

## V
### VLA (Vision-Language-Action)
An integrated system combining computer vision, natural language processing, and robotic action execution to create autonomous systems capable of understanding and responding to voice commands with appropriate physical actions guided by visual perception.

### Vision-Guided Manipulation
The process of using computer vision to guide robot manipulation tasks, creating a perception-action loop that enables robots to interact with objects in their environment based on visual input.

### Voice-to-Action Pipeline
A system that converts spoken commands into actionable robot behaviors, typically involving speech recognition, natural language processing, and command interpretation.